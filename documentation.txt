# Running fmriprep on the cluster
The goal of this document is to help you understand how to run fmriprep on the IKIM cluster

- Slurm will automatically assign nodes to the jobs you submit
- one node will be occupied for the wrapper of the slurm jobs
- it is important to limit the communication of the nodes to the NFS file storage
- the preprocessing of all participants is parallelized only across participants
- therefore 1 job = 1 participant
- to minimize IO traffic, everything is handled locally

Things that will be stored on each node temporarily:
- single sub dataset
- the apptainer sif
- a shell script for submitting the job for the single participant
- the derivatives

Here is the basic outline of the "for loop" to submit the jobs
Iterate over the participants in the NFS folder:
- copy the container to the node
- copy the target participant into a BIDS compatible format (incl. description.json)
- run the actual analysis, and
    - write the temp files locally
    - output the derivatives locally
- when the analysis is complete, copy the derivatives back to the NFS
- delete everything else (temp files, docker container, single-sub BIDS folder)

To handle an upper limit of jobs, so i.e. only 8 simultaneously, we can submit the jobs with a while loop in the
participant level for loop. For this we just check the amount of jobs our USER has currently running, compare that
to the MAX JOBS and either submit a new job (=participant), or we just wait a specific amount and check again, until
all participants are done.

There are two types of log files currently created:
- one automatically named, next to the slurm script which contains the logs of the wrapper that distributes the jobs
- logs for each participant, stored in the provided folder. This holds the logs on the actual analysis pipeline

- only cancel the job that manages the jobs, if the individual jobs die, they at least delete most of the data
- take care of using variables! When using variables inside the for loop, make sure to use "\" so that it is not
immediately evaluated when starting the script, but only in-line
- when trying to access a specific node through slurm to clean up some data for example, you can run the following:
$ srun --time 01:00:00 -w c57 --pty bash -i
- this requests access to c57 for 1 hour, and it automatically starts an interactive terminal

- apptainer automatically binds $HOME and /scratch, this does not work on the cluster, thats why we have to manually
mount the temporary working directory
